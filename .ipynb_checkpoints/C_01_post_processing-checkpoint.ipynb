{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8f0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28cfc4",
   "metadata": {},
   "source": [
    "# Preparing Twitter and mobile phone data from plotting\n",
    "This notebook prepares the metrics that were calculated in the notebooks before for plotting. All the different metrics are put in a similar fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37757df",
   "metadata": {},
   "source": [
    "### Set Variables\n",
    "Define the periods for onsets and offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77bc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonoff1, sonoff2, sonoff3 = dt(2020, 4, 6), dt(2021, 4, 6), dt(2022, 4, 6)\n",
    "eonoff1, eonoff2, eonoff3 = dt(2020, 6, 6), dt(2021, 6, 6), dt(2022, 6, 6)\n",
    "\n",
    "sonoff1, sonoff2, sonoff3 = \"20200406\", \"20210406\", \"20220406\"\n",
    "eonoff1, eonoff2, eonoff3 = \"20200606\", \"20210606\", \"20220606\"\n",
    "\n",
    "pre_subset = \"2020-10-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9edfe",
   "metadata": {},
   "source": [
    "# Post-processing OD-matrices\n",
    "First, the differet OD-matrices are normalized. For the study we need the matrices of the 3 on/offset periods and the matrix of the total study period for twitter and mobile phone data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fced8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfad60f",
   "metadata": {},
   "source": [
    "### Load in Twitter OD-Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ead3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods[\"total_twitter_od\"] = np.load(r\"data\\movement_matrices\\total\\mm_20210603.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4980ec9",
   "metadata": {},
   "source": [
    "### Load in mobile phone data OD-matrix\n",
    "Load in the mobile phone data movement matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2a7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods[\"total_mpd_od\"] = np.load(\"data/movement_matrices/mobile_phone_data/total.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698955af",
   "metadata": {},
   "source": [
    "### Normalize OD-matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05b2bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix, set_diag_to_0=True):\n",
    "    if not matrix.shape[0] == matrix.shape[1]:\n",
    "        raise ValueError(\"WRONG MATRIX SHAPE!!\")\n",
    "        \n",
    "    for i in range(matrix.shape[0]):\n",
    "        matrix[i,i] = 0\n",
    "        \n",
    "    return matrix / matrix.sum()\n",
    "\n",
    "ods_norm = {}\n",
    "\n",
    "# Overwrite the matrices with normalized matrices\n",
    "for key, val in ods.items():\n",
    "    ods_norm[key] = normalize(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd677",
   "metadata": {},
   "source": [
    "### Write Normalized OD-Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7924a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/post_processing/\"\n",
    "\n",
    "for key, val in ods_norm.items():\n",
    "    np.save(path+key+\"_normalized.npy\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb9e07",
   "metadata": {},
   "source": [
    "# Post processing for temporal plots\n",
    "All the metrics from the notebook before are generalized in three manners. Scaling (putting the data between 0 and 1 for comparability), normalizing (changing the values based on tweet amount) and a rolling average (one with 14 and one with 28 days).\n",
    "\n",
    "The data is always scaled, however for experimatal purposes we also caluculated without normalizing or rolling averages. This is adressed in the paper and some example plots can also be found in its appendix.\n",
    "\n",
    "Since the data seems to be influenced by the tweet volume, we also calculated alle the metrics with a fixed amount of tweets. So all the B-notebooks were calculated with a steady sample number of tweets depending on the rolling window size. E.g. for 7-days it was 1115 tweets. This represents the 98th percentile of rolling windows with more than this number of tweets. The effects of this are also discussed in the paper.\n",
    "\n",
    "The main points in the paper however are made with the full tweet volume with scaling, without normalizating (since we could not find enough evidence that the data really is influenced by tweet volume to justify a normalization) and a 28 day moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020b163",
   "metadata": {},
   "source": [
    "### Load in twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10f022d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_twitter = {}\n",
    "# Load only subsets\n",
    "p = \"data/statistics/\"\n",
    "for file in [file for file in os.listdir(p) if ('days' in file)]:\n",
    "    if 'statistics' in file and \"notebookdemo\" not in file:\n",
    "        data = pd.read_csv(p+file)\n",
    "        if str(data['start_date'][0])[-2:] == \".0\":\n",
    "            data['start_date'] = data['start_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d.0\"))\n",
    "        else:\n",
    "            data['start_date'] = data['start_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d\"))\n",
    "        data['middle_date'] = data['middle_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d\"))\n",
    "        data = data.set_index('middle_date')\n",
    "        data = data.drop(columns=[\"Unnamed: 1\"])\n",
    "        statistics_twitter[file.split(\"_\")[1].split(\".\")[0]] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9a1bd",
   "metadata": {},
   "source": [
    "### Load in sample based twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc8d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_twitter_sample = {}\n",
    "# Load only subsets\n",
    "p = \"data/sample_based_statistics/\"\n",
    "for file in [file for file in os.listdir(p) if ('days' in file)]:\n",
    "    if 'statistics' in file:\n",
    "        data = pd.read_csv(p+file)\n",
    "        if str(data['start_date'][0])[-2:] == \".0\":\n",
    "            data['start_date'] = data['start_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d.0\"))\n",
    "        else:\n",
    "            data['start_date'] = data['start_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d\"))\n",
    "        data['middle_date'] = data['middle_date'].apply(lambda x: dt.strptime(str(x), \"%Y%m%d\"))\n",
    "        data = data.set_index('middle_date')\n",
    "        data = data.drop(columns=[\"Unnamed: 1\"])\n",
    "        statistics_twitter_sample[file.split(\"_\")[1].split(\".\")[0]] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabb0e2",
   "metadata": {},
   "source": [
    "### Load in mobile phone data\n",
    "Load in the mpd metrics and rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec4e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"data\\statistics\\mobility_metrics_paper.csv\"\n",
    "mpd = pd.read_csv(data_path, sep=';', decimal=',')\n",
    "mpd['date'] = mpd.date.apply(lambda x: dt.strptime(str(x), \"%Y-%m-%d 00:00:00\"))\n",
    "mpd = mpd.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "979f99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.columns = ['number_unique_users', 'weekly citywide penetration rate', 'no_real_movements', 'jl_simple_means_over_user_means',\\\n",
    "                  'graph_modularity', 'rel_tweets_in_residential_areas', 'mean_rog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cd5e1",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "We define the functions for scaling and normalizing.\n",
    "MPD contains a series of zeros, which were replaced witt nan values to avoid distorting the graph.\n",
    "\n",
    "(Normalizing has to be done in different approaches. Since the graph modularity moves inversely correlated with tweet volume it is multiplied instead of devided and the relative tweet amount in residential areas is independent of tweet amount by nature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70dbe8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale0to1_column(s):\n",
    "    s = copy.deepcopy(s)\n",
    "    column_name = s.name\n",
    "    s[s == 0] = np.nan\n",
    "    s_valid = s[s.notna()]\n",
    "    min_value = s_valid.min()\n",
    "    max_value = s_valid.max()\n",
    "    s = (s - min_value) / (max_value - min_value)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e692d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(sub, folder, key, samples=False):\n",
    "    if not os.path.exists(f'data/post_processing/twitter/{folder}/'):\n",
    "        os.mkdir(f'data/post_processing/twitter/{folder}/')\n",
    "    path = f'data/post_processing/twitter/{folder}/{key}.csv'\n",
    "    \n",
    "    if samples:\n",
    "        if not os.path.exists(f'data/post_processing/sample_based/{folder}/'):\n",
    "            os.mkdir(f'data/post_processing/sample_based/{folder}/')\n",
    "        path = f'data/post_processing/sample_based/{folder}/{key}.csv'\n",
    "    sub.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa46f90",
   "metadata": {},
   "source": [
    "### All rolling window combinations - create and export - also sample based\n",
    "For each rolling window (1 to 31 days) all relevant combinations of scaling normalizing and rolling averages are created and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35562fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'data/post_processing/twitter'):\n",
    "    os.mkdir(f'data/post_processing/twitter')\n",
    "if not os.path.exists(f'data/post_processing/sample_based'):\n",
    "    os.mkdir(f'data/post_processing/sample_based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35b182d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_stats = ['number_unique_users', 'no_real_movements', 'jl_simple_means_over_user_means',\\\n",
    "                  'graph_modularity', 'rel_tweets_in_residential_areas', 'mean_rog']\n",
    "\n",
    "sizes = [28]\n",
    "\n",
    "for stats, samples in zip([statistics_twitter, statistics_twitter_sample], [False, True]):\n",
    "    for key, val in stats.items():\n",
    "        numtweets = val['no_of_tweets']\n",
    "\n",
    "        sub_og = copy.deepcopy(val[wanted_stats])\n",
    "\n",
    "        # only scaled\n",
    "        sub = copy.deepcopy(sub_og)\n",
    "        sub = sub.apply(scale0to1_column)\n",
    "        export(sub, 'scaled', key, samples)\n",
    "\n",
    "        # trend, scaled\n",
    "        sub = copy.deepcopy(sub_og)\n",
    "        for size in sizes:\n",
    "            rm_sub = sub.apply(lambda x: x.rolling(size, center=True).mean())\n",
    "            rm_sub = rm_sub.apply(scale0to1_column)\n",
    "            export(rm_sub, 'trend_scaled', key+\"_\"+str(size), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aefc77",
   "metadata": {},
   "source": [
    "### Trend, then scaled - MPD\n",
    "For mobile phone data, we do not explore every combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ce76e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make trends and scale\n",
    "mpds_trends = {}\n",
    "\n",
    "for size in sizes:\n",
    "    temp = mpd.apply(lambda x: x.rolling(size, center=True).mean())\n",
    "    mpds_trends[str(size)] = temp.apply(scale0to1_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7ed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only scaled, without trend\n",
    "mpd_scaled = mpd.apply(scale0to1_column)\n",
    "mpd_scaled.to_csv('data/post_processing/mpd_scaled.csv', index=True)\n",
    "\n",
    "for key, val in mpds_trends.items():\n",
    "    val.to_csv(f'data/post_processing/mpd_trend{key}_scaled.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ed86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime as dt\n",
    "import backend_codes.get_tweets as gt\n",
    "import backend_codes.tweet_processing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c902a",
   "metadata": {},
   "source": [
    "# Proprocessing Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c766f7e",
   "metadata": {},
   "source": [
    "# Assigning Neighborhoods via Places\n",
    "As mentioned in the previous notebook, Twitter no longer uses direct coordinates for attaching tweet locations. It is now more common to get implicit location data e.g your gym, coffee shop, bus stop, neighborhood, city, state, country, etc.\n",
    "We want to perform an analysis on neighborhood level, so the only interesting place types for us are neighborhoods and POIs. These have to be assigned to the district the are located in.\n",
    "\n",
    "Furthermore, each tweet should have an entry for each of its coordinates as well as these coordinates in WKT format.\n",
    "\n",
    "The geodata for Rios neighborhoods, the Barrios, are used from the [Rio open data portal](LINK). These are the geometries that the tweets are going to be matched to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644dbfb",
   "metadata": {},
   "source": [
    "### Filtering Places\n",
    "First, we load the places list. The code tweet_processing has the possibility to filter tweets or place lists. We initialize an analyzer object, then we use all common filters and finally we seperate POIs from neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = gt.load_tweets(r'data\\tweets\\retrieved_tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bf230b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1 Places\n",
      "0.0025760581158635887 % of the places were located in a city or bigger region\n",
      "\n",
      "\n",
      "Removed 801 places\n",
      "97.93652429285383 % of the places were POIs \n",
      "\n",
      "\n",
      "Removed 38017 places\n",
      "2.0634757071461696 % of the places were Neighborhoods \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "places = fr['includes']['places']\n",
    "\n",
    "place_analyser = tp.PlaceAnalyser(places)\n",
    "places = place_analyser.use_filters(neighborhoods=True, pois=True)\n",
    "neigh = place_analyser.neighborhoods\n",
    "pois = place_analyser.pois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9001f22",
   "metadata": {},
   "source": [
    "### Omitting Neighborhood Tweets\n",
    "Since the are multiple issues with matching tweets that only have a neighborhood tag to the actual neighborhood, we decided to omit these tweets entirely. Some of the 163 neighborhoods for example do not exist on twitter. Others exist but have different geomtries that on the official [open data portal](https://www.data.rio/). Since we could not develop a coherent method, to overcome these problems, we decided to just use the POI tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080436f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "barrios = gpd.read_file('data/shps/neighborhoods.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e505c5",
   "metadata": {},
   "source": [
    "### Matching the Pois to the open data neighborhoods\n",
    "For the pois, twitter just acts like its giving a bounding box, in reality it is just a point with its coordinates doubled:\n",
    "- 'bbox': [-43.39237500933108,\n",
    "  -22.97877033005603,\n",
    "  -43.39237500933108,\n",
    "  -22.97877033005603]\n",
    "  \n",
    "We create a Point from these coordinates and the do a point in polygon test for each barrio. If we find a truth value, we put that into our list, otherwise we put None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6ae7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(pois):\n",
    "    coords = n['geo']['bbox'][:2]\n",
    "    p = Point(coords[0], coords[1])\n",
    "    \n",
    "    cod = None\n",
    "    for j, poly in enumerate(barrios.geometry.values):\n",
    "        if p.within(poly):\n",
    "            cod = barrios.loc[j].CODBAIRRO\n",
    "            break\n",
    "    \n",
    "    pois[i]['cod'] = cod\n",
    "    \n",
    "    p = Point(pois[i]['geo']['bbox'][0], pois[i]['geo']['bbox'][1])\n",
    "\n",
    "    pois[i]['wkt'] = p\n",
    "    pois[i]['lat'] = p.y\n",
    "    pois[i]['lon'] = p.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aea640",
   "metadata": {},
   "source": [
    "The places have to be combined and are then associated with their place_id for matching later. So for each place id as key, there is the place data and the respective cod (the neighborhoods' ID) in the dictionary as value. Many places do not have a cod, since they are outside of rios' boundaries, but inside its bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f02716",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6d0553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i, place in enumerate(places):\n",
    "        \n",
    "    data = None\n",
    "    \n",
    "    # lets also attach the geoinformation\n",
    "    if place['place_type'] == 'poi':\n",
    "        data = [place['wkt'], place['lat'], place['lon']]\n",
    "        \n",
    "    if place['place_type'] == 'neighborhood' and place['cod'] is not None:\n",
    "        data = barrios.loc[barrios.CODBAIRRO == place['cod']][['wkt', 'lat', 'lon']].values.tolist()[0]\n",
    "        \n",
    "    d[place['id']] = (place['cod'], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89b49b",
   "metadata": {},
   "source": [
    "### Adding Barrios to the Tweets\n",
    "Now we have cods for all place-ids that we want, we will now add it to the tweets.\n",
    "\n",
    "All Tweets should have such a place-id attached. We check here for potential mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11d6bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets raw:  699524\n",
      "Deleted 0 Tweets\n",
      "100.0 % were not retweets\n",
      "\n",
      "\n",
      "Removed 3 Tweets\n",
      "99.99957113694455 % of the Tweets have a location attached\n",
      "\n",
      "\n",
      "Removed 1824 Tweets\n",
      "0.26074985597287537 % of the Tweets were coordinates\n",
      "\n",
      "\n",
      "Removed 7 Tweets\n",
      "0.0010033008598213655 % of the Tweets were located in a city or bigger region\n",
      "\n",
      "\n",
      "Removed 1455 Tweets\n",
      "99.79145465751265 % of the Tweets have a place type attached\n",
      "\n",
      "\n",
      "Tweets after first filters:  696235\n"
     ]
    }
   ],
   "source": [
    "tweets = copy.deepcopy(fr['data'])\n",
    "print('Tweets raw: ', len(tweets))\n",
    "tweets = tp.TweetAnalyzer(tweets).use_filters()\n",
    "print('Tweets after first filters: ', len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185c9ed",
   "metadata": {},
   "source": [
    "There are no retweets in the dataset\n",
    "\n",
    "3 tweets have no location attached (likely bug)\n",
    "\n",
    "1824 are coordinates, these are not useful anymore\n",
    "\n",
    "7 tweets were falsely returned with a place_type city or even bigger (likely bug)\n",
    "\n",
    "1455 tweets did not even have a place type (likely bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b1cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cods(tweets):\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        try:\n",
    "            cod = d[tweet['geo']['id']][0]\n",
    "            data = d[tweet['geo']['id']][1]\n",
    "        except:\n",
    "            data = None\n",
    "            cod = '0'\n",
    "        \n",
    "        if 'place_type' not in tweet['geo']:\n",
    "            print('NO PLACE TYPE')\n",
    "            tweets[i]['geo']['place_type'] = 'coordinate'\n",
    "        \n",
    "        tweets[i]['cod'] = cod\n",
    "    \n",
    "        if data is not None:\n",
    "            tweets[i]['wkt'] = data[0]\n",
    "            tweets[i]['lat'] = data[1]\n",
    "            tweets[i]['lon'] = data[2]\n",
    "            \n",
    "        else:\n",
    "            tweets[i]['wkt'] = None\n",
    "            tweets[i]['lat'] = None\n",
    "            tweets[i]['lon'] = None\n",
    "            \n",
    "    return tweets\n",
    "\n",
    "tweets = add_cods(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15601f0",
   "metadata": {},
   "source": [
    "Filter Tweets with only valid CODs. Here we remove tweets that originate from outside the cities boundaries. Only the tweets with a valid COD are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2be743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420518 of originally 696235 could be assigned to a Barrio (60.4 %)\n",
      "\n",
      "Tweets after valid cod filter:  420518\n",
      "Removed 0 Tweets\n",
      "100.0 % of the Tweets were POIs \n",
      "Tweets after poi filter:  420518\n"
     ]
    }
   ],
   "source": [
    "tweets_filtered = []\n",
    "for i in range(len(tweets)-1, -1, -1):\n",
    "    if tweets[i]['cod'] != '0' and tweets[i]['cod'] is not None:\n",
    "        tweets_filtered.append(tweets[i])\n",
    "        \n",
    "print(f'{len(tweets_filtered)} of originally {len(tweets)} could be assigned to a Barrio ({round(len(tweets_filtered) / len(tweets)*100, 1)} %)\\n')\n",
    "\n",
    "print('Tweets after valid cod filter: ', len(tweets_filtered))\n",
    "\n",
    "only_pois = tp.TweetAnalyzer(tweets_filtered).only_pois()\n",
    "\n",
    "print('Tweets after poi filter: ', len(only_pois))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b6639",
   "metadata": {},
   "source": [
    "117488 could not be assigned to a barrio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141d847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['lang', 'public_metrics', 'conversation_id', 'text', 'referenced_tweets', 'geo', 'withheld']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc3e7f",
   "metadata": {},
   "source": [
    "### Only use POIs\n",
    "Our current work focusses on POIs only. A dataframe is created where a unnecessary information is removed and the columns are renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69767a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_df = pd.DataFrame(only_pois)\n",
    "pois_df = pois_df.drop(columns=drops)\n",
    "pois_df['created_at'] = pois_df['created_at'].apply(lambda x: dt.strptime(x, \"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "pois_df['author_id'] = pois_df['author_id'].apply(lambda x: x['id'])\n",
    "pois_df = pois_df.rename(columns = {'author_id': 'User_ID', 'id': 'Tweet_ID', 'geo': 'Place_ID', 'created_at': 'Timestamp'})\n",
    "pois_df.cod = pois_df.cod.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b12744",
   "metadata": {},
   "source": [
    "# Bot Filtering\n",
    "For bot filtering we use a threshold to remove all users that send either more that 50 tweets a day of that are responsible for more that 1 % of total tweet output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b3fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = copy.deepcopy(pois_df)\n",
    "\n",
    "grouped['day'] = grouped.Timestamp.apply(dt.date)\n",
    "\n",
    "# Mean Tweets per day per User larger than 50\n",
    "tpdpu = (grouped.groupby('User_ID').size() / (grouped.day.max() - grouped.day.min()).days)\n",
    "bot_users1 = tpdpu[tpdpu > 50]\n",
    "\n",
    "# More than 1 % of Tweets\n",
    "ppu = (grouped.groupby('User_ID').size() / len(grouped))\n",
    "bot_users2 = ppu[ppu > 0.01]\n",
    "\n",
    "bots = bot_users1 + bot_users2\n",
    "\n",
    "filtered = grouped.loc[~grouped.User_ID.isin(bots.index)]\n",
    "filtered = filtered[pois_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv('data/tweets/preprocessed_tweets_with_poi_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253f4ce",
   "metadata": {},
   "source": [
    "### Show short summary of out Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a533a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Twitter POIs are 420518 Tweets\n",
      "Number of unique users: 107500\n",
      "First POI-Tweet: 2020-04-06 00:03:33\n",
      "Last POI-Tweet: 2022-08-31 23:58:25\n"
     ]
    }
   ],
   "source": [
    "print(\"Our Twitter POIs are {} Tweets\".format(len(pois_df)))\n",
    "print(\"Number of unique users: {}\".format(pois_df.User_ID.nunique()))\n",
    "print(\"First POI-Tweet: {}\".format(pois_df.Timestamp.min()))\n",
    "print(\"Last POI-Tweet: {}\".format(pois_df.Timestamp.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2ac66",
   "metadata": {},
   "source": [
    "### Add tweet counts to neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1c46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = gpd.GeoDataFrame(filtered, geometry=filtered.wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce2934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count(x):\n",
    "    clip = gpd.clip(tweets, x)\n",
    "    return len(clip)\n",
    "\n",
    "barrios['counts'] = barrios['geometry'].apply(_count)\n",
    "barrios['counts_per_pop'] = barrios['counts'] / barrios['popsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a08ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "barrios.to_file('data/shps/neighborhoods.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf7f335",
   "metadata": {},
   "source": [
    "# Getting Tweets from Twitter\n",
    "For data acquisition, the twitter API (academic track) is used. Keep in mind that changes to the twitter API are frequent, especialle since the latest change of management. We already experienced difficulties getting the same data with the same requests. For example, the latest trials give us no more geodata for poi tweets.\n",
    "\n",
    "This notebook presents our workflow for communicating with the API, however in for the following notebooks we use the data acquired at ~September 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1b2b3",
   "metadata": {},
   "source": [
    "### Loading modules\n",
    "The base code for communicating with the Twitter API is get_tweets.py. get_tweets is a comprehesive code for communicating with the twitter API that was written for another project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c4e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import backend_codes.get_tweets as gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ddde9",
   "metadata": {},
   "source": [
    "The get_tweets module operates object orientated, so you create an instance of the getter and set the respective variables. After that you can use different commands to pull the tweets for the API, e.g. get_tweets(), where you specify the number of pages you want to pull."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838094a0",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c8bd5",
   "metadata": {},
   "source": [
    "Start time and end time in ISO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a1be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=\"2020-04-06T00:00:00.000Z\"\n",
    "end_time=\"2022-07-31T23:59:59.000Z\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be694574",
   "metadata": {},
   "source": [
    "Define the bounding box of Rio's city boundaries. One bounding box is too much for the twitter limitations (max ~40 km of edge length), so we work with two bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b53d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox=[(-43.79682, -23.08779, -43.44485, -22.74568),\n",
    "        (-43.44485, -23.08779, -43.09288, -22.74568)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f737c41",
   "metadata": {},
   "source": [
    "### More details on geolocation:\n",
    "Tweets nowadays do not really have real coordinates attached to them (very few still have, but these coordinates are unuseful since the only have a precicion of two decimal points). When users give their locations they do it on different adimistrave levels. Each place, no matter its type has a unique place-id in the twitter database:\n",
    "- POIs: For example the local Gym or Coffee shop. These are given as exact coordinates provided by Twitter\n",
    "- Neighborhoods: For example the centro district in Rio. In this case Twitter gives a bounding box of this district and the district name.\n",
    "- City: The user just attaches the city where they sent the tweet form. This is irrelevant when analysing urban movements inside the city\n",
    "- Even higher adiministrave units (state, country, etc.), also irrelevant\n",
    "\n",
    "Most users who send a geolocated tweet just share the city they sent it from (e.g. Rio de Janeiro, Duque de Caxias, etc.). Since we do not need these Tweets and do not want to strech our tweets limits, we identify the place-ids of Rio itself and all citys that cross Rio's bounding box. This still leaves tweets sent from adjecent cities on neighborhood level, but these are not to many and can be filtered later.\n",
    "\n",
    "Set the place_ids in a list to combine them, put -- in front to exclude them instead of explicitly getting tweets only from this place, use False as the second argument in order to get AND logic in between, so all these cities are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcdcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_id=(['--97bcdfca1a2dca59', '--41bf05f3b26396e4', '--596a82c8c53236bd', \n",
    "            '--4029837e46e8e369', '--1c88433143d383e1', '--59373f0a295160e4', \n",
    "            '--7343e9c57b3427b5', '--3b5c5c9c62f7c538',\n",
    "            '--abbb45debbf38127', '--b1a5f3bbff698d24', '--d1fc0c973adbff22'], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be3aa2",
   "metadata": {},
   "source": [
    "### Initialize a getter object and set parameters\n",
    "has_geo has to be on True, since we only want geolocated tweets, we do not want retweets since the do not produce new location data. Results per page are the number of tweets we get per request sent to twitter, in our case this should be the maximum (500) to save on the request limits.\n",
    "\n",
    "The function returns a short summary of the current request setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a40eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query:\n",
      "\n",
      "place.fields: full_name,geo,id,name,place_type\n",
      "\n",
      "query: (bounding_box:[-43.79682 -23.08779 -43.44485 -22.74568] OR bounding_box:[-43.44485 -23.08779 -43.09288 -22.74568]) (-place:97bcdfca1a2dca59 -place:41bf05f3b26396e4 -place:596a82c8c53236bd -place:4029837e46e8e369 -place:1c88433143d383e1 -place:59373f0a295160e4 -place:7343e9c57b3427b5 -place:3b5c5c9c62f7c538 -place:abbb45debbf38127 -place:b1a5f3bbff698d24 -place:d1fc0c973adbff22) -is:retweet has:geo\n",
      "\n",
      "tweet.fields: created_at,text,public_metrics,referenced_tweets,geo,lang,conversation_id\n",
      "\n",
      "expansions: author_id,geo.place_id\n",
      "\n",
      "max_results: 500\n",
      "\n",
      "start_time: 2020-04-06T00:00:00.000Z\n",
      "\n",
      "end_time: 2022-07-31T23:59:59.000Z\n",
      "\n",
      "=================================\n",
      "Your Query is 400 characters long\n"
     ]
    }
   ],
   "source": [
    "getter = gt.FullArchiveV2()\n",
    "\n",
    "getter.set_parameters(start_time=start_time, end_time=end_time, bbox=bbox, has_geo=True, get_retweets=False, \\\n",
    "                      place_id=place_id, max_results_per_page=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121cb48",
   "metadata": {},
   "source": [
    "### Running the tweet machine\n",
    "get tweets gets the number_of_pages * max_results_per_page tweets. If it is more tweets, we can call get_more_tweets which will send further requests to twitter. The getter automatically attaches the places to the tweet. Twitter however sends the place information as well as user information seperatly from the tweet itself to avoid redundancy. The original tweet response can be accessed via getter.full_response. One request does not always contain 500 tweets, since tweets get for example deleted or are not accessible for another reason.\n",
    "Examples below:\n",
    "\n",
    "If we want to get all possible tweets in this timeframe we have to put a very high number of pages. Since this is a demostration notebook, we cannot pull the tweets here to not reveal the API token. Tweets are loaded in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb594c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = getter.get_tweets(number_of_pages=1, save_inbetween=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f2bbb",
   "metadata": {},
   "source": [
    "### Working with the full response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c32317",
   "metadata": {},
   "source": [
    "The full response contains three fields: data (the tweets), errors (hopefully empty), and includes. Inculdes contains users (all users with their information and id to be joined later with their respective tweets) and places (all places (neighborhoods, pois, etc.), their information and also ids to be joined later with the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4955007",
   "metadata": {},
   "source": [
    "Normally, the full response would be accessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c01eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = getter.full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e6c83",
   "metadata": {},
   "source": [
    "But we can also load the full response with all of our data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12b7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = gt.load_tweets(r\"data\\tweets\\retrieved_tweets.txt\")\n",
    "tweets = full_response['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c90e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi\n",
      "poi\n",
      "poi\n",
      "poi\n",
      "poi\n",
      "poi\n",
      "neighborhood\n",
      "poi\n",
      "poi\n",
      "poi\n"
     ]
    }
   ],
   "source": [
    "for p in sample(tweets, 10):\n",
    "    print(p['geo']['place_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a540eff",
   "metadata": {},
   "source": [
    "### The full response structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9657b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (all the tweets)\n",
      "     ['public_metrics', 'author_id', 'text', 'lang', 'id', 'conversation_id', 'created_at', 'geo']\n",
      "\n",
      "Includes\n",
      "    Users\n",
      "         [...]\n",
      "    Places (all the places)\n",
      "         ['place_type', 'name', 'id', 'full_name', 'geo']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data (all the tweets)\")\n",
    "print(\"    \", [key for key in full_response['data'][7].keys()])\n",
    "print(\"\\nIncludes\")\n",
    "print(\"    Users\\n         [...]\")\n",
    "print(\"    Places (all the places)\")\n",
    "print(\"        \", [key for key in full_response['includes']['places'][5].keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5022f",
   "metadata": {},
   "source": [
    "### How to go from here?\n",
    "All the places in the full response data will now be assigned to a neighborhood. This is not so simple, because we cannot use accurate location coordinates anymore. Joining the neighborhoods to their respective barrios (neighborhoods) well be described in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

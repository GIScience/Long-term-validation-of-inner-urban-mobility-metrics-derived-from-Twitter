{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ca24b1-88de-46c8-b5af-8959624e9891",
   "metadata": {},
   "source": [
    "# Retrieval of individual human movement trajectories and collective OD matrices from mobile phone data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267d158-299f-4ec3-bd15-5f305fd7c4da",
   "metadata": {},
   "source": [
    "This notebook gives a glimpse of how pyspark can be used to retrieve individual human movements trajectories and collective OD matrices from mobile phone data. \n",
    "It can be run in a loop over multiple time intervals using the \"Optional__Bash_command_for_loop.py\" script. The \"Optional__Bash_command_for_loop.py\" script requires a \"timestamps_df.csv\" file as an input. This can be generated via the notebook \"Optional__Retrieval_of_timestamps_from_calldata_filenames_to_run_analysis_in_a_loop.ipynb\". In a loop the \"Optional__Bash_command_for_loop.py\" script will automatically set the start_timestamp and end_timestamp parameters for this notebook. If not run in a loop start and end timestmap can be set manually (as demonstrated below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e540d57-4054-4593-b862-4460ba30e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timestamp parameters manually, when not using a loop\n",
    "start_timestamp = '20210925001002' # example timestamp\n",
    "end_timestamp   = '20210925011002' # example timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa82be-bcc7-4084-8120-18d7bcf9add1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part A: Retrieve human movement transitions between antennas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23b705-f110-4616-a6c6-88fccc9441a1",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be691764-0033-4ea4-8e68-eb0763bf2b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType, LongType, StructField,  TimestampType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10773c54-bc7e-42e4-9653-d87f92598136",
   "metadata": {},
   "source": [
    "Monitor process time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff3e15-ba35-40fd-8785-390b6675b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93504-790e-40a8-9c19-16f754b0a933",
   "metadata": {},
   "source": [
    "### A.0) Load data in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db89516-88fa-433f-94f5-9795ac36cc38",
   "metadata": {},
   "source": [
    "Define path names and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263b523-2dbd-46ad-b24a-2d4a6a9b7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zipped_calldata_on_server_from_notebook_working_path = '../data/0_input_data/calldata/zipped'\n",
    "path_to_store_unzipped_txt_files                             = '../data/0_input_data/calldata/unzipped'\n",
    "amount_of_additional_txt_files_to_unzip_and_read             = 1 # can be set to a higher number, if mobile phone records can appear in datafiles of subsequent hours/days (the timestmap of the filenames does not have to be consistent with the timestmaps of the actual mobile phone records within a .txt file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede2b8a-86c5-4693-b232-e52e78fe646a",
   "metadata": {},
   "source": [
    "Convert timestamps to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe46a1-fb3e-4c48-b972-dd377c6fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = str(start_timestamp)\n",
    "end_timestamp = str(end_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e30eb-a269-4816-ab7b-7ef02aee37ce",
   "metadata": {},
   "source": [
    "#### A.0.0) Unzip data for time intervall of analysis specified by start and end timestamp (especially relevant when handling big data e.g. on a cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18739a4-5d30-4600-964e-11c98b59797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all filenames \n",
    "files_list = os.listdir(path_to_zipped_calldata_on_server_from_notebook_working_path) # get all file names\n",
    "# Filter by prefix\n",
    "files_list = [x for x in files_list if x.startswith('DDD')]\n",
    "# Sort by name\n",
    "files_list.sort()\n",
    "\n",
    "# Subset file name list to relevant time intervall\n",
    "start_index = files_list.index('DDD_21_' + start_timestamp + '.txt.gz') \n",
    "end_index = files_list.index('DDD_21_' + end_timestamp + '.txt.gz')\n",
    "files_list = files_list[start_index:end_index + amount_of_additional_txt_files_to_unzip_and_read]\n",
    "\n",
    "# Unzip raw datafiles within interval of analysis for computation, unzipped files will be removed later\n",
    "for file in files_list:\n",
    "    with gzip.open(os.path.join(path_to_zipped_calldata_on_server_from_notebook_working_path, file) , \"rb\") as gz:\n",
    "            with open(os.path.join(path_to_store_unzipped_txt_files, file[:-3]), \"wb\") as f:\n",
    "                shutil.copyfileobj(gz, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee5fe5-e313-42cc-951c-34a934de8fd1",
   "metadata": {},
   "source": [
    "#### A.0.1) Create Spark session with unzipped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f7555-b846-4d9a-82ab-a13d082fb38d",
   "metadata": {},
   "source": [
    "For the processing of large data sets the code line of \".config(\"spark.driver.memory\", \"16g\")\" should be adjusted to the available RAM on a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32bf80-dde3-45f3-b1ac-6d45e326a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark spark.local.dir\n",
    "import os\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"../data/spark\"\n",
    "\n",
    "# Create a spark session:\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .appName(\"mobility_RJ\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# the schema of raw mobile phone records\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"time\",  StringType(), True, metadata={\"maxlength\":14, \"minlength\":14}) # the metadata is optional\n",
    "    .add(\"user\", StringType(), True, metadata={\"maxlength\":32, \"minlength\":32}) # the metadata is optional\n",
    "    .add(\"zip1\", IntegerType(), True)\n",
    "    .add(\"zip2\", IntegerType(), True)\n",
    "    .add(\"lat\", DoubleType(), True)\n",
    "    .add(\"lon\", DoubleType(), True))\n",
    "\n",
    "# insert metadata\n",
    "from pyspark.sql.functions import col\n",
    "col(\"time\").alias(\"time\", metadata={\"maxlength\":14})\n",
    "col(\"user\").alias(\"time\", metadata={\"maxlength\":32})\n",
    "\n",
    "# specify pyspark options\n",
    "data = spark.read.option(\"mode\", \"DROPMALFORMED\").csv(os.path.join(path_to_store_unzipped_txt_files, '*.txt'), sep=\"|\", schema=schema)\n",
    "\n",
    "# print spark directory\n",
    "print(os.environ['SPARK_LOCAL_DIRS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551c29e-7b1a-42eb-9dda-2880e421cf86",
   "metadata": {},
   "source": [
    "### A.1) Retrieval of individual human movement trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273a433-c5e0-4d0b-8d08-8428ab6f524c",
   "metadata": {},
   "source": [
    "#### A.1.0 Filter data records by timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad4a3b-b6ba-4694-b942-62f10af19716",
   "metadata": {},
   "source": [
    "We filter data points whose timestamp is within the given time intervall. \n",
    "This step is necessary because sometimes there is a mismatch in the data between the split into data files and the contained data points. \n",
    "This allows us to first load a larger amount of data files and then filter away unwanted points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30939d-c11e-456f-9d0b-69b71f6a1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.rdd.filter(lambda row: row[\"time\"] > start_timestamp and row[\"time\"]  < end_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5185f-f9f1-43b6-80f4-70b45c4f2a15",
   "metadata": {},
   "source": [
    "Store some metadata about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073f113-9110-420d-a534-64d803dbe6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata[\"call_count\"] = filtered_data.count() # count amount of phone connections in the data\n",
    "metadata[\"start_timestamp\"] = start_timestamp\n",
    "metadata[\"end_timestamp\"] = end_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e3e82-416f-4a94-a609-d987cbabfe20",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A.1.1 Retrieve and store antenna locations from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0582b88-9fee-4d73-8e7a-c0771d27cb6d",
   "metadata": {},
   "source": [
    "Introduce nice consecutive indices for antennas. To do so, we create a mapping of `hash(lat, lon) -> idx` such that the index `idx` is consecutive across the antennas that are present in the data. The mapping is stored as a Python `dict` on the frontend but also distributed back to the cluster for use in further data transformations. The first step could be done once and loaded from disk when you are sure that all antennas are included. The second step needs to be performed even with the mapping being loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fd132-7388-4d96-ba3a-36d442fbd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "antennas_dict = dict(filtered_data.map(lambda row: (row[\"lat\"], row[\"lon\"])).distinct().zipWithIndex().collect())\n",
    "antennas = spark.sparkContext.broadcast(antennas_dict)\n",
    "\n",
    "# Additionally, we also write the antenna positions in indexed order into a file for further processing:\n",
    "inv_antennas = {i: pos for pos, i in antennas_dict.items()}\n",
    "with open(\"../data/1_intermediate_output/antenna_positions/antenna_positions_\" + end_timestamp + \".csv\", \"w\") as f:\n",
    "    for i in range(len(inv_antennas)):\n",
    "        f.write(f\"{inv_antennas[i][0]}, {inv_antennas[i][1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae041555-8122-4e30-9928-c63f683d28f0",
   "metadata": {},
   "source": [
    "#### A.1.2 Derive human movement trajectories from sequential antenna connection (+ apply inter-event-time (IET) filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4d8de-4901-424a-949a-b5da682325f9",
   "metadata": {},
   "source": [
    "We replace the `lat` and `lon` field of each connection in the original RDD with the index of the antennas and at the same time we will drop unnecessary data. \n",
    "Note that this RDD is never `collect`ed, which means that the entire evaluation is lazy and will be executed in one sweep with the follow-up data transformations. \n",
    "After this transformation, the rows are of the following form: `userid, (timestamp, antennaid)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8142e1-7013-4470-9e82-8978fbb4e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = filtered_data.map(lambda row: ( row[\"user\"],(row[\"time\"], antennas.value[(row[\"lat\"], row[\"lon\"])]),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea553-48c5-4cf7-9c4c-fcae78aa3763",
   "metadata": {},
   "source": [
    "The next transformation is the cornerstone of the analysis as it does the tracking of all users in a single dataset sweep. After the grouping operation, we drop the userid as it is not needed anymore. The data then has the form `List[(timestamp, antenna_id)]` with one row per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5113d-d865-42d1-8974-69d2a6215b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = preprocessed.groupByKey().map(lambda row: row[1].data).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defff129-00cb-4a37-beb5-60c915241df3",
   "metadata": {},
   "source": [
    "Store the amount of unique users in the metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846afeb9-5acc-46f5-a3fb-450499182fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"user_count\"] = grouped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be172ff-7988-4810-af70-aa1a7498127f",
   "metadata": {},
   "source": [
    "Next, we want to identify only transitions between antennas fullfilling special time constraints (measured in seconds). The following function extracts transitions for each single user according to these constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7bd65-547d-4137-aab7-1c1b6bc26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_stay_duration_threshold = 60*15  # Stays at an antenna below this threshold time will be aggregated into subsequent antenna transitions (=15 min)\n",
    "maximum_inter_event_time = 60*60*4  # Transitions with a inter-time event above this threshold will not be considered in the analysis (=4std.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcdb8e-34c1-4125-a0c9-fc2e4bc66b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transitions(filtering=True):\n",
    "    def extractor(events):\n",
    "        try:\n",
    "            # Is this the correct sorting criterion?\n",
    "            sorted_events = sorted(events, key=lambda e: e[0])\n",
    "            time_between_events = [(datetime.strptime(b[0], \"%Y%m%d%H%M%S\") - datetime.strptime(a[0], \"%Y%m%d%H%M%S\")).seconds for a, b in zip(sorted_events[:-1], sorted_events[1:])]\n",
    "            event_time = time_between_events + [minimum_stay_duration_threshold + 1]\n",
    "            antennas = [e[1] for e in sorted_events]\n",
    "            eventtimepairs = tuple(zip(sorted_events, event_time))\n",
    "            if filtering:\n",
    "                eventtimepairs = tuple(filter(lambda item: item[1] > minimum_stay_duration_threshold, eventtimepairs))\n",
    "            ret = []\n",
    "            for ((ts0, a0), _), ((ts1, a1), _) in zip(eventtimepairs[:-1], eventtimepairs[1:]):\n",
    "                inter_event_time = (datetime.strptime(ts1, \"%Y%m%d%H%M%S\") - datetime.strptime(ts0, \"%Y%m%d%H%M%S\")).seconds\n",
    "                if (not filtering) or inter_event_time < maximum_inter_event_time:\n",
    "                    ret.append((a0, a1, inter_event_time))\n",
    "            return ret\n",
    "        except ValueError:\n",
    "            return []\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac54a52-4233-4181-8a9a-12382ac07558",
   "metadata": {},
   "source": [
    "Next, we find all transitions in the dataset. The rows in our dataset are of the form `antenna1, antenna2` with one row per registered transition. Note that we still have not `collect`ed the result! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ea320-a7e8-41ed-aae6-8d9bdc584df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = grouped.flatMap(extract_transitions(filtering=True)).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d67b0a-229b-43f6-9b59-4aee0382781b",
   "metadata": {},
   "source": [
    "Store the amount of IET-filtered and IET-unfiltered transitions in the metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc053cc-1787-43a0-ba5e-7c0f9ebcf655",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"transition_count_unfiltered\"] = grouped.flatMap(extract_transitions(filtering=False)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311729ee-c5d7-4f89-8051-140817ccbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"transition_count_filtered\"] = transitions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abd516-8da8-4015-9879-11c0257fa4a7",
   "metadata": {},
   "source": [
    "Finally, we count the transitions for all pairs of antennas. The `countByValue` operations does an implicit collect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c76e2-4521-445f-8095-ceed6bd68c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_counts = transitions.map(lambda row: (row[0], row[1])).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1af706-b554-4724-b227-37024b224494",
   "metadata": {},
   "source": [
    "Retrieve and store inter event times as stay times at previous antenna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130eef8d-2045-4214-87cb-1d703e9b9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_times = transitions.map(lambda row: (row[0], row[2])).groupByKey().map(lambda row: (row[0], np.mean(row[1].data), np.std(row[1].data), np.count_nonzero(row[1].data))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb54370-1304-44c1-b898-65ae6b9c33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/1_intermediate_output/stay_times_at_antennas/stay_times_at_antennas_\" + end_timestamp + \".csv\", \"w\") as f:\n",
    "    f.write(\"lat, lon, mean, stddev, count\\n\")\n",
    "    for antenna, mean, stddev, count in stay_times:\n",
    "        f.write(f\"{inv_antennas[antenna][0]}, {inv_antennas[antenna][1]}, {mean}, {stddev}, {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12570a3c-c01c-42af-908c-d46e943f3739",
   "metadata": {},
   "source": [
    "These counts can be fed into a dense `numpy` data structure. An error will be returned if no transitions remains after filtering by IET threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdc904-a6b4-41e1-b03a-89d14bdc17f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tower2tower = np.full((len(antennas_dict), len(antennas_dict)), 0)\n",
    "entries = np.array([(i0, i1, v) for (i0, i1), v in transitions_counts.items()])\n",
    "tower2tower[entries[:, 0], entries[:, 1]] = entries[:, 2]\n",
    "np.save(\"../data/1_intermediate_output/tower2tower/tower2tower_\" + end_timestamp + \".npy\", tower2tower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9232e1d-3c29-4372-8bb9-859019071c82",
   "metadata": {},
   "source": [
    "Save metadata with 'end_timestamps' in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f29a7-d7ec-4de8-a836-ed0e4fb3660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame.from_dict(metadata.items())\n",
    "metadata_df.columns = ['metadata', end_timestamp]\n",
    "metadata_df.set_index('metadata', inplace=True)\n",
    "metadata_df.to_csv('../data/1_intermediate_output/metadata/metadata_' + end_timestamp + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a2297-8c4e-4def-a6a7-29fffa637d8c",
   "metadata": {},
   "source": [
    "# Part B) Conversion of Origin-Destination (OD) matrix from antenna to admin scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618c253-eecd-4c93-a333-da891bccdf07",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499353e-dfb1-403a-8dd6-5bdf657cba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "import functools\n",
    "import geojson\n",
    "import geopandas\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shapely.geometry as geo\n",
    "import scipy.spatial as spatial\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import seaborn as sns \n",
    "import networkx as nx\n",
    "import contextily as cx \n",
    "from contextily import add_basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc71f9c-9adf-40ef-a868-51d4bc1e1416",
   "metadata": {},
   "source": [
    "Specify paths and input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81275b29-4d58-4d0d-a122-0ddd1e3cee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_geojson_file = (\"../data/0_input_data/study_region/study_region_RJ.geojson\")  # GeoJSON file with administrative regions used for the analysis\n",
    "antennas_csv_file = \"../data/1_intermediate_output/antenna_positions/antenna_positions_\" + end_timestamp + \".csv\"  # The csv file with antenna locations\n",
    "max_antenna_range = 5000  # The maximum range of an antenna in meters. This can be used to crop infinite Voronoi cells to a reasonable size.\n",
    "epsg_regions = 29193  # The EPSG code of the regions file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fd205-460c-48a9-875e-4b28793e1c43",
   "metadata": {},
   "source": [
    "#### B.0) Create antenna to antenna OD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934590d-b259-474f-92d3-85542839874b",
   "metadata": {},
   "source": [
    "Load admin regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396f213-bf7f-4a31-b11b-270f3e2f11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = geopandas.read_file(regions_geojson_file)\n",
    "regions_df.crs = epsg_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b12f2-c69e-4d89-a17a-86bfcf089633",
   "metadata": {},
   "source": [
    "Load antenna locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77331e3-2876-4ed2-8a58-442ea27d5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "antennas = np.genfromtxt(antennas_csv_file, delimiter=\",\")\n",
    "projection = pyproj.Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg_regions}\", always_xy=False).transform\n",
    "antennas = np.apply_along_axis(lambda row: projection(*row), 1, antennas)\n",
    "antenna_df = geopandas.GeoDataFrame(geometry=[geo.Point(a) for a in antennas]) # create GDF\n",
    "antenna_df.crs = 29193#epsg_regions\n",
    "antenna_df = antenna_df.to_crs(epsg=\"29193\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78318af1-5954-4462-8778-cbadb444b198",
   "metadata": {},
   "source": [
    "Plot study region with antennas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c153c50-0896-4e8a-a2b8-e232e076d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = regions_df.plot(figsize=(20, 8))\n",
    "ax.set_axis_off()\n",
    "ax = antenna_df.plot(ax=ax, color=\"black\", markersize=2, aspect=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109539e8-30a1-4297-8363-126d8d1b2d88",
   "metadata": {},
   "source": [
    "Build method to calculate a Voronoi tesselation and transform to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"Reconstruct infinite Voronoi regions in a\n",
    "    2D diagram to finite regions.\n",
    "    Source:\n",
    "    [https://stackoverflow.com/a/20678647/1595060](https://stackoverflow.com/a/20678647/1595060)\n",
    "    \"\"\"\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()\n",
    "    # Construct a map containing all ridges for a\n",
    "    # given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "            # Compute the missing endpoint of an\n",
    "            # infinite ridge\n",
    "            t = vor.points[p2] - vor.points[p1]  # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + direction * radius\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "        # Sort region counterclockwise.\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:, 1] - c[1], vs[:, 0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "        new_regions.append(new_region.tolist())\n",
    "\n",
    "    return new_regions, np.asarray(new_vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ff168",
   "metadata": {},
   "source": [
    "Create Voronoi tesselation and transform to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d392b05-9fe9-43e4-b23a-2c9f72cf2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "vor = spatial.Voronoi(antennas)\n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "antenna_df[\"voronoi\"] = [geo.Polygon([vertices[i] for i in reg]) for reg in regions]\n",
    "antenna_df = antenna_df.set_geometry(\"voronoi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8498fc-f43a-4858-8406-ddf6572e63ec",
   "metadata": {},
   "source": [
    "As the Voronoi diagram extends beyond our specified geographic region, we now intersect each Voronoi region with the union of all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b969b-f196-493f-b8f8-b4edbb05f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = functools.reduce(lambda a, b: a.union(b), regions_df.geometry, geo.MultiPolygon())\n",
    "antenna_df[\"cut_voronoi\"] = antenna_df.voronoi.intersection(all_regions)\n",
    "antenna_df = antenna_df.set_geometry(\"cut_voronoi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32461354-e1ff-414f-9695-dd726ed3b223",
   "metadata": {},
   "source": [
    "Plot tesselations (therefore drop antennas outside of tesselations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f20dc-8c42-4fe4-b514-37d1b845d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "antenna_df_filt = antenna_df[antenna_df.cut_voronoi.area > 0]\n",
    "ax = antenna_df_filt.plot(figsize=(20, 8))\n",
    "ax.set_axis_off()\n",
    "ax = antenna_df_filt[\"geometry\"].copy().plot(ax=ax, markersize=2, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4c2eb-1785-4a31-be7d-2dd4e325c3fd",
   "metadata": {},
   "source": [
    "Save tesselations in study regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a60d2-b906-4407-8849-8bda5707ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_voronoi = antenna_df_filt['cut_voronoi']\n",
    "cut_voronoi.to_file('../data/1_intermediate_output/antenna_tesselations/tesselations_' + end_timestamp + '.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82ad7d-7621-4918-8550-45739f1a1280",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B.1) Convert tower2tower matrix to admin2admin matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db1109-8728-4dc4-bdc8-e3860c28e891",
   "metadata": {},
   "source": [
    "Calculate the transformation matrices that describe the relation ship between antennas and admin regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66feaf9d-e045-455e-adbd-76e8085842f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin2tower = np.zeros(shape=(antenna_df.shape[0], regions_df.shape[0]))\n",
    "tower2admin = np.zeros(shape=(regions_df.shape[0], antenna_df.shape[0]))\n",
    "for (i, antenna), (j, region) in itertools.product(antenna_df.iterrows(), regions_df.iterrows()):\n",
    "    if not antenna[\"voronoi\"].intersects(region[\"geometry\"]):\n",
    "        continue\n",
    "    inside_area = all_regions.intersection(antenna[\"voronoi\"]).area\n",
    "    outside_area = (antenna[\"voronoi\"].difference(all_regions).intersection(antenna[\"geometry\"].buffer(max_antenna_range)).area)\n",
    "    admin2tower[i, j] = antenna[\"voronoi\"].intersection(region[\"geometry\"]).area / (inside_area + outside_area)\n",
    "    tower2admin[j, i] = (antenna[\"voronoi\"].intersection(region[\"geometry\"]).area/ region[\"geometry\"].area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef7e6e-c4e5-4ec4-a2d1-044c60558b4d",
   "metadata": {},
   "source": [
    "Matrix multiplication to generate admin2admin matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f329f-e7f6-4eef-896f-86b61f16bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tower2tower = np.load('../data/1_intermediate_output/tower2tower/tower2tower_' + end_timestamp + \".npy\")  # Read matrices\n",
    "admin2admin = np.matmul(np.matmul(tower2admin,tower2tower),admin2tower)                                   # Matrix multiplication\n",
    "np.fill_diagonal(admin2admin, 0)                                                                          # Optional: Fill diagonal with zeros\n",
    "np.save(\"../data/1_intermediate_output/admin2admin/admin2admin_\" + end_timestamp + \".npy\", admin2admin)   # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4b148-ecb6-4b62-ba64-e024980be749",
   "metadata": {},
   "source": [
    "# Part C) Delete unzipped calldata files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fbda4-0172-4c8b-b098-21dabbd65249",
   "metadata": {},
   "source": [
    "Delete unzipped calldata files to not run out of memeory on server. If this notebook is run in a loop, the enxt loop will unzip raw calldata file of the next time interval of interest in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3f1fe-4625-4b91-916c-92425ae26f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder = '../data/0_input_data/calldata/unzipped'\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ca72f-b6b2-4879-beb7-a3e324599be7",
   "metadata": {},
   "source": [
    "Monitor runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c50dad-e5e5-4ab2-b8aa-2ba80fd16822",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
